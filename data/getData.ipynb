{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 반복 중\n",
      "start :: 식품 & 비타민 & 건강 & 푸드 & 제품 & 음식\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee9545e8a5a43048777430aff366334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9061b975ccd8454cb91b3947f39386e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23da6d70832d42f6b0bfc474bc73ffae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb613e663984ad1a31510392fe4b000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68f75fa2775451d80a96ed9c6c77e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc97538296be45b28ae7678302dd7fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb346c4ee17045149713bc81db9f9222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bec7c2c639454f935b31325d73b449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e562bc8b8384803abf89a46b2188544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cc8357ee224a7dba3fb9af92b2bc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c6a68187ee48a98841b9b3c74e5e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e5481fa2c24054b7c67f2cf98c316f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2167cb60e46548cc8c61621906da0cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Preprocessing', max=10.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd3685a074b4f2f958f697006e80205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Preprocessing', max=10.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "end :: 식품 & 비타민 & 건강 & 푸드 & 제품 & 음식\n"
     ]
    }
   ],
   "source": [
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "naver_id = \"\"\n",
    "naver_pw = \"\"\n",
    "import datetime\n",
    "from pyspark import SparkContext\n",
    "import glob\n",
    "today = datetime.date.today()\n",
    "content_file_name = \"content_data/naver_news_content_today\".replace(\"today\",str(today))\n",
    "title_file_name = \"temp_data/naver_news_title_today\".replace(\"today\",str(today))\n",
    "sc.stop()\n",
    "sc = SparkContext(master=\"local\", appName=\"spark app\")\n",
    "\n",
    "for i in range(1):\n",
    "    print(str(i+1) + \"번째 반복 중\" )\n",
    "    get_data(client_id, client_secret, naver_id, naver_pw, content_file_name, sc)\n",
    "    get_model()\n",
    "    get_topics()\n",
    "    save_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(client_id, client_secret, naver_id, naver_pw, content_file_name, sc):\n",
    "    import os\n",
    "    import sys\n",
    "    import urllib.request\n",
    "    import requests\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from selenium import webdriver\n",
    "    from tqdm import notebook\n",
    "    import pickle\n",
    "    import re\n",
    "    import ast\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib\n",
    "    import time\n",
    "\n",
    "    news_data = []\n",
    "    page_count = 10\n",
    "\n",
    "    keywords_arr = sc.textFile(glob.glob(\"keyword/keyword*\")[(len(glob.glob(\"keyword/keyword*\"))-1)]).collect()\n",
    "    keywords = \"\"\n",
    "    for v in keywords_arr:\n",
    "        keywords += v\n",
    "\n",
    "    print(\"start :: \" + keywords)\n",
    "    encText = urllib.parse.quote(keywords)\n",
    "\n",
    "    for idx in notebook.tqdm(range(page_count)):\n",
    "        url = \"https://openapi.naver.com/v1/search/news?query=\" + encText + \"&start=\" + str(idx *10 +1)\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "\n",
    "        if(rescode==200):\n",
    "            #response_body = response.read()\n",
    "            result = requests.get(response.geturl(),\n",
    "                             headers={\"X-Naver-Client-Id\":client_id,\"X-Naver-Client-Secret\":client_secret})\n",
    "            news_data.append(result.json())\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "        time.sleep(1)    \n",
    "\n",
    "\n",
    "    naver_news_link = []\n",
    "\n",
    "    for page in news_data:\n",
    "        page_news_link=[]\n",
    "        for item in page['items']:\n",
    "            link = item['link']\n",
    "            #page_news_link.append(link)\n",
    "            if \"naver\" in link:\n",
    "                page_news_link.append(link)\n",
    "        naver_news_link.append(page_news_link)        \n",
    "\n",
    "    link_count = 0        \n",
    "    for page in naver_news_link:\n",
    "         for link in page:\n",
    "                link_count += 1 \n",
    "\n",
    "    naver_news_title = []\n",
    "    naver_news_content = []\n",
    "\n",
    "    for n in notebook.tqdm(range(len(naver_news_link))):\n",
    "        news_page_title = []\n",
    "        news_page_content = []\n",
    "\n",
    "        if n==0 or n%5 == 0:\n",
    "            chrome_options = webdriver.ChromeOptions()\n",
    "            chrome_options.add_argument('--no-sandbox')\n",
    "            chrome_options.add_argument('--window-size=1420,1080')\n",
    "            chrome_options.add_argument('--headless')\n",
    "            chrome_options.add_argument('--disable-gpu')\n",
    "            chrome_options.add_argument('--disable-dev-shm-usage')        \n",
    "            driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "            driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "            driver.find_element_by_name('id').send_keys(naver_id)\n",
    "            driver.find_element_by_name('pw').send_keys(naver_pw)\n",
    "\n",
    "        for idx in notebook.tqdm(range(len(naver_news_link[n]))):\n",
    "            time.sleep(2)\n",
    "\n",
    "            try:\n",
    "\n",
    "                driver.get(naver_news_link[n][idx])\n",
    "            except:\n",
    "                print(\"Timeout!\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = driver.page_source\n",
    "\n",
    "            except UnexpectedAlertPresentException:\n",
    "                driver.switch_to_alert().accept()\n",
    "                print(\"게시글이 삭제됨\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "            title = None\n",
    "\n",
    "            try:\n",
    "                item = soup.find('div', class_=\"article_info\")\n",
    "                title = item.find('h3', class_=\"tts_head\").get_text()\n",
    "            except:\n",
    "                title = \"OUTLINK\"\n",
    "\n",
    "            news_page_title.append(title)\n",
    "\n",
    "            doc = None\n",
    "            text= \"\"\n",
    "\n",
    "            data = soup.find_all(\"div\",{\"class\" : \"_article_body_contents\"})\n",
    "            if data:\n",
    "                for item in data:\n",
    "                    text = text + str(item.find_all(text=True)).strip()\n",
    "                    text = ast.literal_eval(text)\n",
    "                    doc = ' '.join(text)\n",
    "            else:\n",
    "                doc = \"OUTLINK\"\n",
    "\n",
    "            news_page_content.append(doc.replace('\\n',' '))\n",
    "\n",
    "        naver_news_title.append(news_page_title)\n",
    "        naver_news_content.append(news_page_content)\n",
    "\n",
    "    filecount=len(glob.glob(\"content_data/*\"))\n",
    "    sc.parallelize(naver_news_content).saveAsTextFile(content_file_name+\"_\"+str(filecount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "    mecab = Mecab()\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    # notebook.tqdm\n",
    "    from konlpy.tag import Mecab\n",
    "    import string\n",
    "    import warnings\n",
    "    from gensim import corpora\n",
    "    from gensim import models\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "    # documents=[]\n",
    "    # for filename in glob.glob(\"content_data/*\"):\n",
    "    #    documents += sc.textFile(filename).collect()\n",
    "    documents = sc.textFile(glob.glob(\"content_data/*\")[(len(glob.glob(\"content_data/*\"))-1)]).collect()    \n",
    "    SW = set()\n",
    "    for i in string.punctuation:\n",
    "        SW.add(i)\n",
    "\n",
    "    with open(\"stopwords-ko.txt\") as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "\n",
    "    cleaned_text = []\n",
    "    for doc in documents:\n",
    "        temp_doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힇 ]\",\"\",doc)    \n",
    "        cleaned_text.append(temp_doc)\n",
    "\n",
    "\n",
    "    mecab = Mecab()\n",
    "    tokenized_text = []\n",
    "    tokenizer=\"noun\"\n",
    "    if tokenizer == \"noun\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.nouns(cleaned_text[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    elif tokenized == \"morph\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.morphs(cleaned_text[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    elif tokenized == \"word\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = cleaned_text[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=link_count, id2word=dictionary)\n",
    "\n",
    "    #model.show_topic(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics():\n",
    "    NUM_TOPICS = link_count \n",
    "    NUM_TOPIC_WORDS = 30 \n",
    "\n",
    "    def build_doc_term_mat(documents):\n",
    "        dictionary = corpora.Dictionary(documents)\n",
    "        corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "        return corpus, dictionary\n",
    "\n",
    "\n",
    "    documents=[]\n",
    "    #for filename in glob.glob(\"content_data/*\"):\n",
    "        #documents += sc.textFile(filename).collect()\n",
    "    \n",
    "    documents += sc.textFile(glob.glob(\"content_data/*\")[(len(glob.glob(\"content_data/*\"))-1)]).collect()    \n",
    "    #documents = read_documents(content_file_name)\n",
    "    SW = define_stopwords(\"stopwords-ko.txt\")\n",
    "    cleaned_text = text_cleaning(documents)\n",
    "    tokenized_text = text_tokenizing(cleaned_text, tokenizer=\"noun\")\n",
    "\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=link_count, id2word=dictionary)\n",
    "\n",
    "    # document-term matrix\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "    corpus = [dictionary.doc2bow(document) for document in tokenized_text]\n",
    "\n",
    "    # LDA 실행\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=dictionary, alpha=\"auto\", eta=\"auto\")\n",
    "\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "\n",
    "    data = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "    filecount=len(glob.glob(\"html_results/*\"))\n",
    "    pyLDAvis.save_html(data, 'html_results/lda_'+str(filecount)+'.html')\n",
    "    #data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_topics():\n",
    "    temp = data.topic_info.head(6)['Term'].values\n",
    "    topic_words =\"\"\n",
    "    for i,v in enumerate(temp):\n",
    "        if i == len(temp)-1:\n",
    "            topic_words += v \n",
    "        else:\n",
    "            topic_words += v + \" & \"\n",
    "    print(\"end :: \" + topic_words)\n",
    "\n",
    "    filecount=len(glob.glob(\"keyword/keyword*\"))\n",
    "    sc.parallelize(topic_words).saveAsTextFile(\"keyword/keyword_\"+str(filecount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
