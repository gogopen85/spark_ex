{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=spark app, master=local) created by __init__ at <ipython-input-4-b3d5cb1ddd9f>:12 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-626013e0eff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtitle_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"temp_data/naver_news_title_today\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"today\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#sc.stop()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spark app\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"건강 & 섭취 & 음식\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=spark app, master=local) created by __init__ at <ipython-input-4-b3d5cb1ddd9f>:12 "
     ]
    }
   ],
   "source": [
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "naver_id = \"\"\n",
    "naver_pw = \"\"\n",
    "import datetime\n",
    "from pyspark import SparkContext\n",
    "import glob\n",
    "today = datetime.date.today()\n",
    "content_file_name = \"content_data/naver_news_content_today\".replace(\"today\",str(today))\n",
    "title_file_name = \"temp_data/naver_news_title_today\".replace(\"today\",str(today))\n",
    "#sc.stop()\n",
    "sc = SparkContext(master=\"local\", appName=\"spark app\")\n",
    "\n",
    "keywords = \"건강 & 섭취 & 음식\"\n",
    "\n",
    "for i in range(3):\n",
    "    print(str(i+1) + \"번째 반복 중\" )\n",
    "    link_count = get_data(client_id, client_secret, naver_id, naver_pw, content_file_name, sc, keywords)\n",
    "    get_model(link_count)\n",
    "    get_topics(link_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(client_id, client_secret, naver_id, naver_pw, content_file_name, sc, keywords):\n",
    "    import os\n",
    "    import sys\n",
    "    import urllib.request\n",
    "    import requests\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from selenium import webdriver\n",
    "    from tqdm import notebook\n",
    "    import pickle\n",
    "    import re\n",
    "    import ast\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib\n",
    "    import time\n",
    "\n",
    "    news_data = []\n",
    "    \n",
    "   \n",
    "    page_count = 3\n",
    "    \n",
    "    if len(glob.glob(\"keyword/keyword*\")) > 0 :\n",
    "        keywords_arr = sc.textFile(glob.glob(\"keyword/keyword*\")[(len(glob.glob(\"keyword/keyword*\"))-1)]).collect()\n",
    "        keywords = \"\"\n",
    "        for v in keywords_arr:\n",
    "            keywords += v\n",
    "    else:\n",
    "        filecount=len(glob.glob(\"keyword/keyword*\"))\n",
    "        sc.parallelize(keywords).saveAsTextFile(\"keyword/keyword_\"+str(filecount))\n",
    "        \n",
    "    print(\"start :: \" + keywords)\n",
    "    encText = urllib.parse.quote(keywords)\n",
    "    \n",
    "    for idx in notebook.tqdm(range(page_count)):\n",
    "        url = \"https://openapi.naver.com/v1/search/news?query=\" + encText + \"&start=\" + str(idx *10 +1)\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "\n",
    "        if(rescode==200):\n",
    "            #response_body = response.read()\n",
    "            result = requests.get(response.geturl(),\n",
    "                             headers={\"X-Naver-Client-Id\":client_id,\"X-Naver-Client-Secret\":client_secret})\n",
    "            news_data.append(result.json())\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "            break\n",
    "        time.sleep(1)    \n",
    "\n",
    "\n",
    "    naver_news_link = []\n",
    "\n",
    "    for page in news_data:\n",
    "        page_news_link=[]\n",
    "        for item in page['items']:\n",
    "            link = item['link']\n",
    "            #page_news_link.append(link)\n",
    "            if \"naver\" in link:\n",
    "                page_news_link.append(link)\n",
    "        naver_news_link.append(page_news_link)        \n",
    "\n",
    "    link_count = 0        \n",
    "    for page in naver_news_link:\n",
    "         for link in page:\n",
    "                link_count += 1 \n",
    "\n",
    "    naver_news_title = []\n",
    "    naver_news_content = []\n",
    "    \n",
    "    for n in notebook.tqdm(range(len(naver_news_link))):\n",
    "        news_page_title = []\n",
    "        news_page_content = []\n",
    "\n",
    "        if n==0 or n%5 == 0:\n",
    "            chrome_options = webdriver.ChromeOptions()\n",
    "            chrome_options.add_argument('--no-sandbox')\n",
    "            chrome_options.add_argument('--window-size=1420,1080')\n",
    "            chrome_options.add_argument('--headless')\n",
    "            chrome_options.add_argument('--disable-gpu')\n",
    "            chrome_options.add_argument('--disable-dev-shm-usage')        \n",
    "            driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "            driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "            driver.find_element_by_name('id').send_keys(naver_id)\n",
    "            driver.find_element_by_name('pw').send_keys(naver_pw)\n",
    "\n",
    "        for idx in notebook.tqdm(range(len(naver_news_link[n]))):\n",
    "            time.sleep(2)\n",
    "\n",
    "            try:\n",
    "\n",
    "                driver.get(naver_news_link[n][idx])\n",
    "            except:\n",
    "                print(\"Timeout!\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = driver.page_source\n",
    "\n",
    "            except UnexpectedAlertPresentException:\n",
    "                driver.switch_to_alert().accept()\n",
    "                print(\"게시글이 삭제됨\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "            title = None\n",
    "\n",
    "            try:\n",
    "                item = soup.find('div', class_=\"article_info\")\n",
    "                title = item.find('h3', class_=\"tts_head\").get_text()\n",
    "            except:\n",
    "                title = \"OUTLINK\"\n",
    "\n",
    "            news_page_title.append(title)\n",
    "\n",
    "            doc = None\n",
    "            text= \"\"\n",
    "\n",
    "            data = soup.find_all(\"div\",{\"class\" : \"_article_body_contents\"})\n",
    "            if data:\n",
    "                for item in data:\n",
    "                    text = text + str(item.find_all(text=True)).strip()\n",
    "                    text = ast.literal_eval(text)\n",
    "                    doc = ' '.join(text)\n",
    "            else:\n",
    "                doc = \"OUTLINK\"\n",
    "\n",
    "            news_page_content.append(doc.replace('\\n',' '))\n",
    "\n",
    "        naver_news_title.append(news_page_title)\n",
    "        naver_news_content.append(news_page_content)\n",
    "\n",
    "    filecount=len(glob.glob(\"content_data/*\"))\n",
    "    sc.parallelize(naver_news_content).saveAsTextFile(content_file_name+\"_\"+str(filecount))\n",
    "    \n",
    "    return link_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(link_count):\n",
    "    from konlpy.tag import Mecab\n",
    "    import re\n",
    "    from tqdm import notebook\n",
    "\n",
    "    mecab = Mecab()\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    # notebook.tqdm\n",
    "    from konlpy.tag import Mecab\n",
    "    import string\n",
    "    import warnings\n",
    "    from gensim import corpora\n",
    "    from gensim import models\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "    # documents=[]\n",
    "    # for filename in glob.glob(\"content_data/*\"):\n",
    "    #    documents += sc.textFile(filename).collect()\n",
    "    documents = sc.textFile(glob.glob(\"content_data/*\")[(len(glob.glob(\"content_data/*\"))-1)]).collect()    \n",
    "    SW = set()\n",
    "    for i in string.punctuation:\n",
    "        SW.add(i)\n",
    "\n",
    "    with open(\"stopwords-ko.txt\") as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "\n",
    "    cleaned_text = []\n",
    "    for doc in documents:\n",
    "        temp_doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힇 ]\",\"\",doc)    \n",
    "        cleaned_text.append(temp_doc)\n",
    "\n",
    "\n",
    "    mecab = Mecab()\n",
    "    tokenized_text = []\n",
    "    tokenizer=\"noun\"\n",
    "    if tokenizer == \"noun\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.nouns(cleaned_text[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    elif tokenized == \"morph\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.morphs(cleaned_text[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    elif tokenized == \"word\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = cleaned_text[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=link_count, id2word=dictionary)\n",
    "     \n",
    "    #model.show_topic(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(link_count):\n",
    "    import string\n",
    "    import re\n",
    "    from konlpy.tag import Mecab\n",
    "    from tqdm import notebook\n",
    "    from gensim import corpora\n",
    "    from gensim import models\n",
    "\n",
    "    \n",
    "    NUM_TOPICS = link_count \n",
    "    NUM_TOPIC_WORDS = 30 \n",
    "\n",
    "    def build_doc_term_mat(documents):\n",
    "        dictionary = corpora.Dictionary(documents)\n",
    "        corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "        return corpus, dictionary\n",
    "\n",
    "\n",
    "    documents=[]\n",
    "    #for filename in glob.glob(\"content_data/*\"):\n",
    "        #documents += sc.textFile(filename).collect()\n",
    "    \n",
    "    documents += sc.textFile(glob.glob(\"content_data/*\")[(len(glob.glob(\"content_data/*\"))-1)]).collect()    \n",
    "    #documents = read_documents(content_file_name)\n",
    "    \n",
    "    #SW = define_stopwords(\"stopwords-ko.txt\")\n",
    "    SW = set()\n",
    "    for i in string.punctuation:\n",
    "        SW.add(i)\n",
    "\n",
    "    with open(\"stopwords-ko.txt\") as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "    \n",
    "    cleaned_text = []\n",
    "    for doc in documents:\n",
    "        temp_doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힇 ]\",\"\",doc)    \n",
    "        cleaned_text.append(temp_doc)\n",
    "    \n",
    "    mecab = Mecab()\n",
    "    tokenized_text = []\n",
    "    tokenizer=\"noun\"\n",
    "    if tokenizer == \"noun\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.nouns(cleaned_text[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    elif tokenized == \"morph\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.morphs(cleaned_text[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    elif tokenized == \"word\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = cleaned_text[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    \n",
    "\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=link_count, id2word=dictionary)\n",
    "\n",
    "    # document-term matrix\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "    corpus = [dictionary.doc2bow(document) for document in tokenized_text]\n",
    "\n",
    "    # LDA 실행\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=dictionary, alpha=\"auto\", eta=\"auto\")\n",
    "\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "\n",
    "    data = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "    filecount=len(glob.glob(\"html_results/*\"))\n",
    "    pyLDAvis.save_html(data, 'html_results/lda_'+str(filecount)+'.html')\n",
    "    temp = data.topic_info.head(6)['Term'].values\n",
    "    topic_words =\"\"\n",
    "    for i,v in enumerate(temp):\n",
    "        if i == len(temp)-1:\n",
    "            topic_words += v \n",
    "        else:\n",
    "            topic_words += v + \" | \"\n",
    "    print(\"end :: \" + topic_words)\n",
    "\n",
    "    filecount=len(glob.glob(\"keyword/keyword*\"))\n",
    "    sc.parallelize(topic_words).saveAsTextFile(\"keyword/keyword_\"+str(filecount))\n",
    "    \n",
    "    #data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
